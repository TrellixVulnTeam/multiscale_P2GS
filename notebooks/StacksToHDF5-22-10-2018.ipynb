{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was created on October 4th, 2018\n",
    "\n",
    "The idea is to be able to upload Tif files (which are different time points with the z-stacks stacked in one same tiff) and convert them into HDF5 format for using them later with BigData Viewer from Fiji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py \n",
    "from glob import glob\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we get filenames for our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input = 'E:/Test-Arianne-SPIM-DATA/SPIM/Channel1Stacked'\n",
    "path_output = 'E:/Test-Arianne-SPIM-DATA/SPIM/Channel1Stacked/HDF5_Python'\n",
    "filenames = sorted(glob('%s/*.tif'%path_input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and inspect a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (180, 2048, 2048)\n",
      "Type:  uint16\n"
     ]
    }
   ],
   "source": [
    "im = imread(filenames[0]) # Sample image\n",
    "print('Shape: ',im.shape)\n",
    "print('Type: ',im.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dimensions of the Raw data\n",
    "z_dim = im.shape[0]\n",
    "y_dim = im.shape[1]\n",
    "x_dim = im.shape[2]\n",
    "t_dim = len(filenames)\n",
    "channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an HDF5 file and an HDF5 dataset called '/x' within that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_HDF5 = 'SPIMChannel1_2' # How will the HDF5 be called\n",
    "f = h5py.File('%s/%s.hdf5'%(path_output,name_HDF5),'a') # Make an HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Perform Downsamplings in X-Y-Z Dimension\n",
    "def downSample(a, resolution):\n",
    "    window_z, window_y, window_x = resolution\n",
    "    z, x, y = a.shape\n",
    "    xr = np.arange(0, x, window_x)\n",
    "    yr = np.arange(0, y, window_y)\n",
    "    zr = np.arange(0, z, window_z)\n",
    "\n",
    "    return np.add.reduceat(np.add.reduceat(np.add.reduceat(im, zr, axis=0, dtype=np.int16), \\\n",
    "                                                             yr, axis=1, dtype=np.int16), xr, axis = 2, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform splitting into small 3D blocks: Chunks\n",
    "def cellChunk(a, subdivision):\n",
    "    zi, yi, xi = subdivision\n",
    "    z_dim, y_dim, x_dim = a.shape\n",
    "    # Number of 3D Arrays (chunks)\n",
    "    chunks = int((z_dim/zi)*(y_dim/yi)*(x_dim/xi))\n",
    "    \n",
    "    cell = np.zeros((zi, yi, xi, chunks))\n",
    "    count = 0\n",
    "    for x in range(int(x_dim/xi)):\n",
    "        for y in range(int(y_dim/yi)):\n",
    "            for z in range(int(z_dim/zi)):\n",
    "                cell[:,:,:,count] = a[z*zi:zi*(z+1), y*yi:yi*(y+1), x*xi:xi*(x+1)]\n",
    "                count += 1\n",
    "                \n",
    "    return cell, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = f.create_group('__DATA_TYPES__')\n",
    "dt = h5py.special_dtype(enum=('i8',{'FALSE': 0, 'TRUE': 1}))\n",
    "data_type.create_dataset('Enum_Boolean',dtype=dt)\n",
    "\n",
    "\n",
    "dp = h5py.special_dtype(vlen=str)\n",
    "data_type.create_dataset('String_VariableLength', dtype=dp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = np.array([[1,1,1], [1,2,2],[2,4,4],[4,8,8]])\n",
    "subdivision = np.array([[4,32,32], [16,16,16],[16,16,16],[16,16,16]])\n",
    "for i in range(channels):\n",
    "    sSS = f.create_group('s%0.2d'%i)\n",
    "    #sub = sSS.create_group('subdivisions')\n",
    "    sub_sub = sSS.create_dataset('subdivisions',data=subdivision, dtype=np.int32)\n",
    "    #resol = sSS.create_group('resolutions')\n",
    "    sub_resol = sSS.create_dataset('resolutions',data=resolution, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(3):\n",
    "    # Load each tif stack\n",
    "    im = imread(filenames[t]) # Sample image\n",
    "    \n",
    "    # Create the groups where each time point will be saved\n",
    "    tTTTT = f.create_group('t%0.4d'%(t+1))\n",
    "    \n",
    "    for s in range(channels):\n",
    "        # Create thr subgroup for each source (e.g. channel)\n",
    "        sSS = tTTTT.create_group('s%0.2d'%s)\n",
    "        # Create the arrays for subdivisions and resolutions\n",
    "        \n",
    "        # Perform the downsamplings\n",
    "        for i in range(np.shape(resolution)[0]): \n",
    "            # Mipmap level\n",
    "            L = sSS.create_group('%d'%i)\n",
    "            aux_ds = downSample(im, resolution[i])\n",
    "            \n",
    "            # Chunk the downsampled data\n",
    "            #cells, chunks = cellChunk(im, subdivision[i])\n",
    "\n",
    "            L.create_dataset('cells', data=aux_ds, chunks=(subdivision[i][0],subdivision[i][1],subdivision[i][2]),\\\n",
    "                             compression='gzip', compression_opts=6, scaleoffset=0)\n",
    "f.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert our images one at a time into the HDF5 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = f.require_dataset('/x',shape=(len(filenames),im.shape[0],im.shape[1],im.shape[2]), dtype=im.dtype)\n",
    "\n",
    "for i,fn in enumerate(filenames):\n",
    "    im = imread(fn)\n",
    "    out[i,:,:,:] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link the XML to the HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlfh = open('%s/SPIMChannel1_hdf5.xml'%path, 'rb')\n",
    "h5f.attrs['xml'] = xmlfh.read()\n",
    "h5f.attrs['xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldata = \"\"\"<xml>\n",
    "<something>\n",
    "    <else>Text</else>\n",
    "</something>\n",
    "</xml>\n",
    "\"\"\"\n",
    "\n",
    "# Write the xml file...\n",
    "f = h5py.File('%s/%s.hdf5'%(path,name_HDF5)) # Make an HDF5 file\n",
    "str_type = h5py.new_vlen(str)\n",
    "ds = f.create_dataset('%s/SPIMChannel1_hdf5.xml'%path, shape=(1,), dtype=str_type)\n",
    "ds[:] = xmldata\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
